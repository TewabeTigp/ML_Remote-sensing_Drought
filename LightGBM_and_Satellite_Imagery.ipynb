{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcJn8seVhFU/XJf/rg8CHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TewabeTigp/ML_Remote-sensing_Drought/blob/main/LightGBM_and_Satellite_Imagery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How to Predict Harmful Algal Blooms Using LightGBM and Satellite Imagery**"
      ],
      "metadata": {
        "id": "uq8JYNlDQ7r3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inland water bodies like lakes and reservoirs provide critical drinking water and recreation for communities, and habitats for marine life. A significant challenge that water quality managers face is the formation of harmful algal blooms (HABs) such as cyanobacteria. HABs produce toxins that are poisonous to humans and their pets, and threaten marine ecosystems by blocking sunlight and oxygen. Manual water sampling, or “in situ” sampling, is generally used to monitor cyanobacteria in inland water bodies. In situ sampling is accurate, but time intensive and difficult to perform continuously.\n",
        "\n",
        "The goal is to use satellite imagery to detect and classify the severity of cyanobacteria blooms in small, inland water bodies like reservoirs."
      ],
      "metadata": {
        "id": "0QFVLAkqRKSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "f4u-lqlPRkVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import odc.stac\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NnXyUPoESCcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll download all of the competition data to a local folder.\n",
        "\n",
        "The contents of the data folder are:"
      ],
      "metadata": {
        "id": "A0w47eJLSF4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ".\n",
        "├── metadata.csv\n",
        "├── submission_format.csv\n",
        "└── train_labels.csv"
      ],
      "metadata": {
        "id": "nbjYladGSPI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = Path.cwd().parent.resolve() / \"data/final/public\"\n",
        "assert DATA_DIR.exists()"
      ],
      "metadata": {
        "id": "1Ki3SPDPSTiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explore the data**"
      ],
      "metadata": {
        "id": "UNfwZOTVSfAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels in this competition are based on “in situ” samples that were collected manually and then analyzed for cyanobacteria density. Each measurement is a unique combination of date and location (latitude and longitude).\n",
        "\n",
        "**metadata.csv**\n",
        "metadata.csv includes all of the sample measurements in the data. It has the following columns:\n",
        "\n",
        "**uid (str)**: unique ID for each row. Each row is a unique combination of date\n",
        "and location (latitude and longitude).\n",
        "\n",
        "**date (pd.datetime)**: date when the sample was collected, in the format YYYY-MM-DD\n",
        "\n",
        "**latitude (float):** latitude of the location where the sample was collected\n",
        "\n",
        "**longitude (float):** longitude of the location where the sample was collected\n",
        "\n",
        "**region (str):** region of the US. This will be used in scoring to calculate region-specific RMSEs. Final score will be the average across the four US regions.\n",
        "\n",
        "**split (str):** indicates whether the row is part of the train set or the test set. Metadata is provided for all points in the train and test sets.\n",
        "\n",
        "The geographic points in the train and test sets are distinct. This means that none of the test set points are also in the train set, so your model's performance will be measured on unseen locations.\n",
        "\n",
        "The main feature data for this competition is satellite imagery from Sentinel-2 and Landsat. Participants will access all feature data through external, publicly available APIs. Relevant imagery can be identified using the location and date of each sample from the metadata."
      ],
      "metadata": {
        "id": "HutrNXq8SkEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv(DATA_DIR / \"metadata.csv\")\n",
        "metadata.head()"
      ],
      "metadata": {
        "id": "1PQUWXY9SjJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSfacI4zPJs"
      },
      "outputs": [],
      "source": [
        "metadata.split.value_counts(dropna=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Location**\n",
        "\n",
        "Let's take a look at the distribution of samples by location."
      ],
      "metadata": {
        "id": "fWjqBcqfUQdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# load the default geopandas base map file to plot points on\n",
        "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))"
      ],
      "metadata": {
        "id": "HRHpUlDRUHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(9, 4))\n",
        "\n",
        "# map the training data\n",
        "base = world[world.name == \"United States of America\"].plot(\n",
        "    edgecolor=\"gray\", color=\"ghostwhite\", figsize=(9, 4), alpha=0.3, ax=ax\n",
        ")\n",
        "train_meta = metadata[metadata[\"split\"] == \"train\"]\n",
        "geometry = [Point(xy) for xy in zip(train_meta[\"longitude\"], train_meta[\"latitude\"])]\n",
        "gdf = gpd.GeoDataFrame(train_meta, geometry=geometry)\n",
        "gdf.plot(ax=base, marker=\".\", markersize=3, color=\"blue\", label=\"Train\", alpha=0.6)\n",
        "\n",
        "# map the test data\n",
        "test_meta = metadata[metadata[\"split\"] == \"test\"]\n",
        "geometry = [Point(xy) for xy in zip(test_meta[\"longitude\"], test_meta[\"latitude\"])]\n",
        "gdf = gpd.GeoDataFrame(test_meta, geometry=geometry)\n",
        "gdf.plot(ax=base, marker=\".\", markersize=3, color=\"orange\", label=\"Test\", alpha=0.6)\n",
        "\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.xlim([-125, -65])\n",
        "plt.ylim([25, 50])\n",
        "plt.legend(loc=4, markerscale=3)"
      ],
      "metadata": {
        "id": "DIMDXOP3UYVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:We can see that the sampling points are distributed all throughout the continental United States."
      ],
      "metadata": {
        "id": "CuYJ-_jsUz28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date**\n",
        "\n",
        "What date range are the samples from?"
      ],
      "metadata": {
        "id": "btI-IFtFU33U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert date to pd.datetime\n",
        "metadata.date = pd.to_datetime(metadata.date)\n",
        "\n",
        "# what is the date range?\n",
        "metadata.groupby(\"split\").agg(min_date=(\"date\", min), max_date=(\"date\", max))"
      ],
      "metadata": {
        "id": "adsvLg7wUoTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what years are in the data?\n",
        "pd.crosstab(metadata.date.dt.year, metadata.split).plot(kind=\"bar\")\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.title(\"Distribution of years in the data\")"
      ],
      "metadata": {
        "id": "Br6etEwJU_tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what seasons are the data points from?\n",
        "metadata[\"season\"] = (\n",
        "    metadata.date.dt.month.replace([12, 1, 2], \"winter\")\n",
        "    .replace([3, 4, 5], \"spring\")\n",
        "    .replace([6, 7, 8], \"summer\")\n",
        "    .replace([9, 10, 11], \"fall\")\n",
        ")\n",
        "metadata.season.value_counts()"
      ],
      "metadata": {
        "id": "cXF7XYfgVQgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**Most of the data is from summer. Harmful algal blooms are more likely to be dangerous during the summer because more individuals are taking advantage of water bodies like lakes for recreation."
      ],
      "metadata": {
        "id": "qnaODL6IVRzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# where is data from for each season?\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
        "\n",
        "for season, ax in zip(metadata.season.unique(), axes.flatten()):\n",
        "    base = world[world.name == \"United States of America\"].plot(\n",
        "        edgecolor=\"gray\", color=\"ghostwhite\", alpha=0.3, ax=ax\n",
        "    )\n",
        "\n",
        "    sub = metadata[metadata.season == season]\n",
        "    geometry = [Point(xy) for xy in zip(sub[\"longitude\"], sub[\"latitude\"])]\n",
        "    gdf = gpd.GeoDataFrame(sub, geometry=geometry)\n",
        "    gdf.plot(ax=base, marker=\".\", markersize=2.5)\n",
        "    ax.set_xlim([-125, -66])\n",
        "    ax.set_ylim([25, 50])\n",
        "    ax.set_title(f\"{season.capitalize()} data points\")\n",
        "    ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "Efx210S-VaZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **train_labels.csv**\n",
        "Let's look at the labels for the training data."
      ],
      "metadata": {
        "id": "EYr4K_ZBV1AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_csv(DATA_DIR / \"train_labels.csv\")\n",
        "train_labels.head()"
      ],
      "metadata": {
        "id": "D4TGhm2cV9mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "8hmVqH3XWC10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have one row per in situ sample. Each row is a unique combination of date and location (latitude + longitude). There are columns for:\n",
        "\n",
        "**uid (str):** unique ID for each row. The uid maps each row in train_labels.csv to metadata.csv\n",
        "\n",
        "**region (str):** US region in which the sample was taken. Scores are calculated separately for each of these regions, and then averaged. See the Problem Description page for details.\n",
        "\n",
        "**severity (int):** severity level based on the cyanobacteria density. This is what you'll be predicting.\n",
        "\n",
        "**density (float):** raw measurement of total cyanobacteria density in cells per milliliter (mL)\n",
        "\n",
        "\n",
        "Note that the target value for participants to predict a severity level, NOT raw cell density in cells per mL. The severity levels are:\n",
        "\n",
        "\n",
        "***************************\n",
        "\n",
        "We can find the matching metadata for each training label by merging on uid. For example:"
      ],
      "metadata": {
        "id": "44H0MaL2WRmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_and_metadata = train_labels.merge(\n",
        "    metadata, how=\"left\", left_on=\"uid\", right_on=\"uid\", validate=\"1:1\"\n",
        ")"
      ],
      "metadata": {
        "id": "XUA_jHVHWl79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cyanobacteria values**\n",
        "\n",
        "What are the cyanobacteria severity values like?"
      ],
      "metadata": {
        "id": "Bvlj_ALIXmZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "severity_counts = (\n",
        "    train_labels.replace(\n",
        "        {\n",
        "            \"severity\": {\n",
        "                1: \"1 (<20,000)\",\n",
        "                2: \"2 (20,000-100,000)\",\n",
        "                3: \"3 (100,000 - 1,000,000)\",\n",
        "                4: \"4 (1,00,000 - 10,000,000)\",\n",
        "                5: \"5 (>10,000,00)\",\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    .severity.value_counts()\n",
        "    .sort_index(ascending=False)\n",
        ")\n",
        "plt.barh(severity_counts.index, severity_counts.values)\n",
        "plt.xlabel(\"Number of samples\")\n",
        "plt.ylabel(\"Severity (range in cells/mL)\")\n",
        "plt.title(\"Train labels severity level counts\")"
      ],
      "metadata": {
        "id": "2nbM7kqBXe9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.density.describe()"
      ],
      "metadata": {
        "id": "tjYFhvXAXwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the density is highly skewed to the right, with a long tail of very high values.\n",
        "\n",
        "There are also some rows where the density of cyanobacteria is 0."
      ],
      "metadata": {
        "id": "NQjhhOsqX0_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_labels.density == 0).sum()"
      ],
      "metadata": {
        "id": "7XZu9e7gX8Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **submission_format.csv**\n",
        "Now let's look at the submission format. To create your own submission, download the submission format and replace the severity column with your own prediction values. **To be scored correctly, the order of your predictions must match the order of uids in the submission format.**"
      ],
      "metadata": {
        "id": "dhGE9PIJX4k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_format = pd.read_csv(DATA_DIR / \"submission_format.csv\", index_col=0)\n",
        "submission_format.head()"
      ],
      "metadata": {
        "id": "SuPdWnDpYFqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_format.shape"
      ],
      "metadata": {
        "id": "tYaGnueMYPxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the training data, each row is a unique combination of location and date. The columns in submission_format.csv are:\n",
        "\n",
        "**uid (str):** unique ID for each row. The uid maps each row in train_labels.csv to metadata.csv\n",
        "\n",
        "**region (str):** US region in which the sample was taken. Scores are calculated separately for each of these regions, and then averaged. See the Problem Description page for details.\n",
        "\n",
        "**severity (int):** placeholder for severity level based on the cyanobacteria density - all values are 0. This is the column that you will replace with your own predictions to create a submission. **Participants should submit predictions for severity level, NOT for the raw cell density value in cells per mL.**"
      ],
      "metadata": {
        "id": "bZ__f5PhYXjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process feature data**"
      ],
      "metadata": {
        "id": "rAJLGG28Y5tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature data is not provided through the competition site. Instead, participants will access all feature data through external, publicly available APIs. Relevant imagery can be identified using the location and date of each sample, listed in metadata.csv.\n",
        "\n",
        "In this section, we'll walk through the process of pulling in matching satellite imagery using Microsoft's Planetary Computer. We'll only use Sentinel-2 L2A and Landsat Level-2 satellite imagery. Sentinel-2 L1C and Landsat Level-1 imagery are also allowed. For links to more guides on how to pull feature data, see the Problem Description page. There are a number of optional sources in addition to satellite imagery that you can also experiment with!\n",
        "\n",
        "**The general steps we'll use to pull satellite data are:**\n",
        "\n",
        "1. Establish a connection to the Planetary Computer's STAC API using the planetary_computer and pystac_client Python packages.\n",
        "\n",
        "2. Query the STAC API for scenes that capture our in situ labels. For each sample, we'll search for imagery that includes the sample's location (latitude and longitude) around the date the sample was taken. In this benchmark, we'll use only Sentinel-2 L2A and Landsat Level-2 data.\n",
        "\n",
        "3. Select one image for each sample. We'll use Sentinel-2 data wherever it is available, because it is higher resolution. We'll have to use Landsat for data before roughly 2016, because Sentinel-2 was not available yet.\n",
        "\n",
        "4. Convert the image to a 1-dimensional list of features that can be input into our tree model\n",
        "\n",
        "We'll first walk through these steps for just one example measurement. Then, we'll refactor the process and run it on all of our metadata."
      ],
      "metadata": {
        "id": "sgljvEqvZCuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish a connection to the STAC API\n",
        "import planetary_computer as pc\n",
        "from pystac_client import Client\n",
        "\n",
        "catalog = Client.open(\n",
        "    \"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=pc.sign_inplace\n",
        ")"
      ],
      "metadata": {
        "id": "Hqi5IdGuYfhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Walk through pulling feature data for one sample**"
      ],
      "metadata": {
        "id": "AK6cogo_ZwCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the STAC API"
      ],
      "metadata": {
        "id": "eYL6QhAUZybS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_row = metadata[metadata.uid == \"garm\"].iloc[0]\n",
        "example_row"
      ],
      "metadata": {
        "id": "QrNZGJSzaBIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can search (41.98006, -110.65734) in Google maps to see example where this sample is from. Turns out it's from Lake Viva Naughton in Wyoming!\n",
        "\n",
        "**Location range:** We'll search an area with 50,000 meters on our either side of our sample point (100,000 m x 100,000 m), to make sure we're pulling in all relevant imagery. This is just a starting point, and you can improve on how to best search for the correct location in the Planetary Computer."
      ],
      "metadata": {
        "id": "4z6F9NrHaG2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopy.distance as distance\n",
        "# get our bounding box to search latitude and longitude coordinates\n",
        "def get_bounding_box(latitude, longitude, meter_buffer=50000):\n",
        "    \"\"\"\n",
        "    Given a latitude, longitude, and buffer in meters, returns a bounding\n",
        "    box around the point with the buffer on the left, right, top, and bottom.\n",
        "\n",
        "    Returns a list of [minx, miny, maxx, maxy]\n",
        "    \"\"\"\n",
        "    distance_search = distance.distance(meters=meter_buffer)\n",
        "\n",
        "    # calculate the lat/long bounds based on ground distance\n",
        "    # bearings are cardinal directions to move (south, west, north, and east)\n",
        "    min_lat = distance_search.destination((latitude, longitude), bearing=180)[0]\n",
        "    min_long = distance_search.destination((latitude, longitude), bearing=270)[1]\n",
        "    max_lat = distance_search.destination((latitude, longitude), bearing=0)[0]\n",
        "    max_long = distance_search.destination((latitude, longitude), bearing=90)[1]\n",
        "\n",
        "    return [min_long, min_lat, max_long, max_lat]\n",
        "\n",
        "\n",
        "bbox = get_bounding_box(example_row.latitude, example_row.longitude, meter_buffer=50000)\n",
        "bbox"
      ],
      "metadata": {
        "id": "dJWw0oWXaDyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time range:** We want our feature data to be as close to the time of the sample as possible, because in algal blooms in small water bodies form and move very rapidly. Remember, you cannot use any data collected after the date of the sample.\n",
        "\n",
        "Imagery taken with roughly 10 days of the sample will generally still be an accurate representation of environmental conditions at the time of the sample. For some data points you may not be able to get data within 10 days, and may have to use earlier data. We'll search the fifteen days up to the sample time, including the sample date."
      ],
      "metadata": {
        "id": "CDoDhmY6agTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get our date range to search, and format correctly for query\n",
        "def get_date_range(date, time_buffer_days=15):\n",
        "    \"\"\"Get a date range to search for in the planetary computer based\n",
        "    on a sample's date. The time range will include the sample date\n",
        "    and time_buffer_days days prior\n",
        "\n",
        "    Returns a string\"\"\"\n",
        "    datetime_format = \"%Y-%m-%dT\"\n",
        "    range_start = pd.to_datetime(date) - timedelta(days=time_buffer_days)\n",
        "    date_range = f\"{range_start.strftime(datetime_format)}/{pd.to_datetime(date).strftime(datetime_format)}\"\n",
        "\n",
        "    return date_range\n",
        "\n",
        "\n",
        "date_range = get_date_range(example_row.date)\n",
        "date_range"
      ],
      "metadata": {
        "id": "97PRotjBalVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search the planetary computer sentinel-l2a and landsat level-2 collections\n",
        "search = catalog.search(\n",
        "    collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"], bbox=bbox, datetime=date_range\n",
        ")\n",
        "\n",
        "# see how many items were returned\n",
        "items = [item for item in search.get_all_items()]\n",
        "len(items)"
      ],
      "metadata": {
        "id": "NQD6QAz4av6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Select one image**\n",
        "\n",
        "The planetary computer returned 46 different items! Let's look at some of the details of the items that were found to match our label.\n",
        "\n",
        "Remember that our example measurement was taken on 2021-09-27 at coordinates (41.98006, -110.65734). Because we used a bounding box around the sample to search, the Planetary Computer returned all items that contain any part of that bounding box. This means we still have to double check whether each item actually contains our sample point."
      ],
      "metadata": {
        "id": "2nTODElma53i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get details of all of the items returned\n",
        "item_details = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"datetime\": item.datetime.strftime(\"%Y-%m-%d\"),\n",
        "            \"platform\": item.properties[\"platform\"],\n",
        "            \"min_long\": item.bbox[0],\n",
        "            \"max_long\": item.bbox[2],\n",
        "            \"min_lat\": item.bbox[1],\n",
        "            \"max_lat\": item.bbox[3],\n",
        "            \"bbox\": item.bbox,\n",
        "            \"item_obj\": item,\n",
        "        }\n",
        "        for item in items\n",
        "    ]\n",
        ")\n",
        "\n",
        "# check which rows actually contain the sample location\n",
        "item_details[\"contains_sample_point\"] = (\n",
        "    (item_details.min_lat < example_row.latitude)\n",
        "    & (item_details.max_lat > example_row.latitude)\n",
        "    & (item_details.min_long < example_row.longitude)\n",
        "    & (item_details.max_long > example_row.longitude)\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Filtering from {len(item_details)} returned to {item_details.contains_sample_point.sum()} items that contain the sample location\"\n",
        ")\n",
        "\n",
        "item_details = item_details[item_details[\"contains_sample_point\"]]\n",
        "item_details[[\"datetime\", \"platform\", \"contains_sample_point\", \"bbox\"]].sort_values(\n",
        "    by=\"datetime\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZCx7fE0Ra0V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple in this benchmark, we'll just choose one to input into our benchmark model. Note that in your solution, you could find a way to incorporate multiple images!\n",
        "\n",
        "We'll narrow to one image in two steps:\n",
        "\n",
        "1. If any Sentinel imagery is available, filter to only Sentinel imagery. Sentinel-2 is higher resolution than Landsat, which is extremely helpful for blooms in small water bodies. In this case, two images are from Sentinel and contain the actual sample location.\n",
        "\n",
        "2. Select the item that is the closest time wise to the sampling date. This gives us a Sentinel-2A item that was captured on 10/20/2022 - two days before our sample was collected on 10/22."
      ],
      "metadata": {
        "id": "5gYuOyBybka5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - filter to sentinel\n",
        "item_details[item_details.platform.str.contains(\"Sentinel\")]"
      ],
      "metadata": {
        "id": "9AI5G15zbqUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - take closest by date\n",
        "best_item = (\n",
        "    item_details[item_details.platform.str.contains(\"Sentinel\")]\n",
        "    .sort_values(by=\"datetime\", ascending=False)\n",
        "    .iloc[0]\n",
        ")\n",
        "best_item"
      ],
      "metadata": {
        "id": "EfpDyRPibvzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a Sentinel-2A item that was captured on 9/24/2021 -- three days before our sample was collected on 9/27/2021."
      ],
      "metadata": {
        "id": "BpneaiRab6f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item = best_item.item_obj"
      ],
      "metadata": {
        "id": "zzdZlFaPbzDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This item comes with a number of \"assets\". Let's see what these are."
      ],
      "metadata": {
        "id": "HSsk15StcA6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What assets are available?\n",
        "for asset_key, asset in item.assets.items():\n",
        "    print(f\"{asset_key:<25} - {asset.title}\")"
      ],
      "metadata": {
        "id": "qHh685ABb-v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have visible bands (red, green, and blue), as well as a number of other spectral ranges and a few algorithmic bands. The Sentinel-2 mission guide has more details about what these bands are and how to use them!\n",
        "\n",
        "A few of the algorithmic bands that may be useful are:\n",
        "\n",
        "1. Scene classification (SCL): The scene classification band sorts pixels into categories including water, high cloud probability, medium cloud probability, and vegetation. Water pixels could be used to calculate the size of a given water body, which impacts the behavior of blooms. Vegetation can indicate non-toxic marine life like sea grass that sometimes resembles cyanobacteria.\n",
        "\n",
        "2. Cloud masks (CLM): Sentinel-2’s cloud bands can be used to filter out cloudy pixels. Pixels that are obscured by clouds are likely not helpful for algal bloom detection because the actual water surface is not captured."
      ],
      "metadata": {
        "id": "tJvUcL3BcR8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing Sentinel-2 imagery**\n",
        "Let's see what the imagery actually looks like."
      ],
      "metadata": {
        "id": "9XIKBg_LcdZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rioxarray\n",
        "from IPython.display import Image\n",
        "from PIL import Image as PILImage"
      ],
      "metadata": {
        "id": "gyWGknsjcFzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the whole image\n",
        "img = Image(url=item.assets[\"rendered_preview\"].href, width=500)\n",
        "\n",
        "Image(url=item.assets[\"rendered_preview\"].href, width=500)"
      ],
      "metadata": {
        "id": "gVEsSyzAckJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a LOT of area, and all we care about is right around our sampling point. Let's get a closer to look to make sure we're in the right neighborhood."
      ],
      "metadata": {
        "id": "yeLIB1bycrNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_sentinel_image(item, bounding_box):\n",
        "    \"\"\"\n",
        "    Given a STAC item from Sentinel-2 and a bounding box tuple in the format\n",
        "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
        "    imagery in the bounding box.\n",
        "\n",
        "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
        "    \"\"\"\n",
        "    (minx, miny, maxx, maxy) = bounding_box\n",
        "\n",
        "    image = rioxarray.open_rasterio(pc.sign(item.assets[\"visual\"].href)).rio.clip_box(\n",
        "        minx=minx,\n",
        "        miny=miny,\n",
        "        maxx=maxx,\n",
        "        maxy=maxy,\n",
        "        crs=\"EPSG:4326\",\n",
        "    )\n",
        "\n",
        "    return image.to_numpy()"
      ],
      "metadata": {
        "id": "Oz0ScHDGcnr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a smaller geographic bounding box\n",
        "minx, miny, maxx, maxy = get_bounding_box(\n",
        "    example_row.latitude, example_row.longitude, meter_buffer=3000\n",
        ")\n",
        "\n",
        "# get the zoomed in image array\n",
        "bbox = (minx, miny, maxx, maxy)\n",
        "zoomed_img_array = crop_sentinel_image(item, bbox)\n",
        "\n",
        "zoomed_img_array[0]"
      ],
      "metadata": {
        "id": "MfTSnKCjc4gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have to transpose some of the dimensions to plot\n",
        "# matplotlib expects channels in a certain order\n",
        "plt.imshow(np.transpose(zoomed_img_array, axes=[1, 2, 0]))"
      ],
      "metadata": {
        "id": "xwPuu9YLc9CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure this imagery looks right, we can search for the sample in Google maps with (latitude, longitude) -- in this case, we'll search (41.98006, -110.65734). That's a match!"
      ],
      "metadata": {
        "id": "sVWVQx-fdA6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing Landsat imagery**\n",
        "STAC items from Sentinel and from Landsat are slightly different, so we'll have to use slightly different methods to pull imagery based on its satellite source.\n",
        "\n",
        "Let's look at one of the Landsat items returned and see how to visualize it. We'll use the odc-stac (https://odc-stac.readthedocs.io/en/latest/_api/odc.stac.load.html#odc.stac.load)"
      ],
      "metadata": {
        "id": "cOUSEvz6dHAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landsat_item = (\n",
        "    item_details[item_details.platform.str.contains(\"landsat\")]\n",
        "    .sample(n=1, random_state=3)\n",
        "    .iloc[0]\n",
        ")\n",
        "landsat_item"
      ],
      "metadata": {
        "id": "wBrnJTzJdCy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_landsat_image(item, bounding_box):\n",
        "    \"\"\"\n",
        "    Given a STAC item from Landsat and a bounding box tuple in the format\n",
        "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
        "    imagery in the bounding box.\n",
        "\n",
        "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
        "    \"\"\"\n",
        "    (minx, miny, maxx, maxy) = bounding_box\n",
        "\n",
        "    image = odc.stac.stac_load(\n",
        "        [pc.sign(item)], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
        "    ).isel(time=0)\n",
        "    image_array = image[[\"red\", \"green\", \"blue\"]].to_array().to_numpy()\n",
        "\n",
        "    # normalize to 0 - 255 values\n",
        "    image_array = cv2.normalize(image_array, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "xx7lqlOJdp0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item = landsat_item.item_obj\n",
        "\n",
        "# we'll use the same cropped area as above\n",
        "landsat_image_array = crop_landsat_image(item, bbox)\n",
        "landsat_image_array[0]"
      ],
      "metadata": {
        "id": "MpyDLRmtd6Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.transpose(landsat_image_array, axes=[1, 2, 0]))"
      ],
      "metadata": {
        "id": "3vSWrEk5d7JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that unlike Sentinel-2 imagery, Landsat imagery is not originally returned with image values scaled to 0-255. Our function above scales the pixel values with cv2.normalize(image_array, None, 0, 255, cv2.NORM_MINMAX) so that it is more comparable to our Sentinel-2 imagery, and we can input both of them as features into our model. You may want to explore other methods of converting Sentinel and Landsat imagery to comparable scales to make sure that no information is lost when re-scaling.\n",
        "\n",
        "For an example, let's look at the Landsat image for the item shown above before it is rescaled to 0-255."
      ],
      "metadata": {
        "id": "qk4wfBDheJbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load image but don't convert to numpy or rescale\n",
        "image = odc.stac.stac_load(\n",
        "    [pc.sign(item)], bands=[\"red\", \"green\", \"blue\"], bbox=bbox\n",
        ").isel(time=0)\n",
        "image_array = image[[\"red\", \"green\", \"blue\"]].to_array()\n",
        "\n",
        "# values are not scaled 0 - 255 when first returned\n",
        "image_array[0]"
      ],
      "metadata": {
        "id": "bJj46Fk0eF0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image appears differently without rescaling\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "image_array.plot.imshow(robust=True, ax=ax)"
      ],
      "metadata": {
        "id": "pJcv7pthePj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert imagery to features**\n",
        "We still have to convert the satellite imagery from 3-dimensional arrays into 1-dimensional features that can be input into our tree model. Here, we'll go with a very simple approach:\n",
        "\n",
        "1. Crop each image to a very small area right around the sample location\n",
        "\n",
        "2. Take the average and median of values for each color band in the image. Each image pixel has a value for red, green, and blue, so this will give us six features per image to put into our model: average red, average green, average blue, median red, median green, and median blue\n",
        "\n",
        "There is LOTS of room to improve on the method of converting images to features! You could test out different ways to aggregate the color bands, like mean and standard deviation instead of mean and median. Another option is to use a pretrained convolutional neural network to do feature extraction with a library like [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/advanced/transfer_learning.html)."
      ],
      "metadata": {
        "id": "VeocWEYCecRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a small area to crop around\n",
        "# crop to 400 meters squared around the sampling point\n",
        "minx, miny, maxx, maxy = get_bounding_box(\n",
        "    example_row.latitude, example_row.longitude, meter_buffer=100\n",
        ")\n",
        "minx, miny, maxx, maxy"
      ],
      "metadata": {
        "id": "e8O7wrm5eUi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = (minx, miny, maxx, maxy)\n",
        "feature_image_array = crop_sentinel_image(best_item.item_obj, bounding_box=bbox)\n",
        "\n",
        "plt.imshow(np.transpose(feature_image_array, axes=[1, 2, 0]))"
      ],
      "metadata": {
        "id": "Z3TsAUccep4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(feature_image_array), feature_image_array.shape"
      ],
      "metadata": {
        "id": "T8jV06X3esx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our image saved as a numpy array with dimensions 3 x 170 x 137. The first dimension is color (or band), and the second and third are the shape of the image. We have a 170 x 137 array for each color band: red, green, and blue.\n",
        "\n",
        "For each color band, we'll take the average value -- each of these averages will go into our model as a feature.\n",
        "\n",
        "Colors may also be highly skewed by other bright objects like clouds. In addition to the average of each band, we'll also take the median to reflect any significant skew.\n",
        "\n",
        "To get our final list of features for the image that will go into our tree model, we'll concatenate the color band averages and medians."
      ],
      "metadata": {
        "id": "Hmkz-bmnewKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take the average over the second and third dimensions\n",
        "image_color_averages = feature_image_array.mean(axis=(1, 2)).tolist()\n",
        "\n",
        "# also take the median\n",
        "image_color_medians = np.median(feature_image_array, axis=(1, 2)).tolist()\n",
        "\n",
        "# concatenate the two lists\n",
        "image_features = image_color_averages + image_color_medians\n",
        "image_features"
      ],
      "metadata": {
        "id": "Xp0xIEwxezPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the full process to get from one sample to a list of features ready for modeling!"
      ],
      "metadata": {
        "id": "IZhnXW8ye9yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Refactor and run on all training data**"
      ],
      "metadata": {
        "id": "ozmx8A0-fAzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consolidate the code above into organized functions, so that we can easily iterate over the training samples and pull in matching satellite items for each sample.\n",
        "\n",
        "Be aware that querying the Planetary Computer for all of the images is going to take a LONG time! You'll want to find ways to both save work as you go, and optimize your code for efficiency."
      ],
      "metadata": {
        "id": "Py_6W_yhfGEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refactor our process from above into functions\n",
        "def select_best_item(items, date, latitude, longitude):\n",
        "    \"\"\"\n",
        "    Select the best satellite item given a sample's date, latitude, and longitude.\n",
        "    If any Sentinel-2 imagery is available, returns the closest sentinel-2 image by\n",
        "    time. Otherwise, returns the closest Landsat imagery.\n",
        "\n",
        "    Returns a tuple of (STAC item, item platform name, item date)\n",
        "    \"\"\"\n",
        "    # get item details\n",
        "    item_details = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                \"datetime\": item.datetime.strftime(\"%Y-%m-%d\"),\n",
        "                \"platform\": item.properties[\"platform\"],\n",
        "                \"min_long\": item.bbox[0],\n",
        "                \"max_long\": item.bbox[2],\n",
        "                \"min_lat\": item.bbox[1],\n",
        "                \"max_lat\": item.bbox[3],\n",
        "                \"item_obj\": item,\n",
        "            }\n",
        "            for item in items\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # filter to items that contain the point location, or return None if none contain the point\n",
        "    item_details[\"contains_sample_point\"] = (\n",
        "        (item_details.min_lat < latitude)\n",
        "        & (item_details.max_lat > latitude)\n",
        "        & (item_details.min_long < longitude)\n",
        "        & (item_details.max_long > longitude)\n",
        "    )\n",
        "    item_details = item_details[item_details[\"contains_sample_point\"] == True]\n",
        "    if len(item_details) == 0:\n",
        "        return (np.nan, np.nan, np.nan)\n",
        "\n",
        "    # add time difference between each item and the sample\n",
        "    item_details[\"time_diff\"] = pd.to_datetime(date) - pd.to_datetime(\n",
        "        item_details[\"datetime\"]\n",
        "    )\n",
        "\n",
        "    # if we have sentinel-2, filter to sentinel-2 images only\n",
        "    item_details[\"sentinel\"] = item_details.platform.str.lower().str.contains(\n",
        "        \"sentinel\"\n",
        "    )\n",
        "    if item_details[\"sentinel\"].any():\n",
        "        item_details = item_details[item_details[\"sentinel\"] == True]\n",
        "\n",
        "    # return the closest imagery by time\n",
        "    best_item = item_details.sort_values(by=\"time_diff\", ascending=True).iloc[0]\n",
        "\n",
        "    return (best_item[\"item_obj\"], best_item[\"platform\"], best_item[\"datetime\"])\n",
        "\n",
        "\n",
        "def image_to_features(image_array):\n",
        "    \"\"\"\n",
        "    Convert an image array of the form (color band, height, width) to a\n",
        "    1-dimensional list of features. Returns a list where the first three\n",
        "    values are the averages of each color band, and the second three\n",
        "    values are the medians of each color band.\n",
        "    \"\"\"\n",
        "    averages = image_array.mean(axis=(1, 2)).tolist()\n",
        "    medians = np.median(image_array, axis=(1, 2)).tolist()\n",
        "\n",
        "    return averages + medians"
      ],
      "metadata": {
        "id": "n5LumWhve2My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll save out the processed features for each image as we go, to make sure we only have to run time-intensive parts of our code once.\n",
        "\n",
        "For time, here we'll train on a randomly selected small subset of the training data. The cell below is highly time intensive, because for each row in the data we have to query the planetary computer catalog and process imagery.\n",
        "\n",
        "We'll still include and predict on all of the test data."
      ],
      "metadata": {
        "id": "YNbHJ8G9fQWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARK_DATA_DIR = DATA_DIR.parents[1] / \"benchmark\"\n",
        "\n",
        "# save image arrays in case we want to generate more features\n",
        "IMAGE_ARRAY_DIR = BENCHMARK_DATA_DIR / \"image_arrays\"\n",
        "IMAGE_ARRAY_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "Fl_s33gwfSe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a random subset of the training data for the benchmark\n",
        "train_subset = metadata[metadata[\"split\"] == \"train\"].sample(n=2500, random_state=2)\n",
        "\n",
        "# combine train subset with all test data\n",
        "metadata_subset = pd.concat([train_subset, metadata[metadata[\"split\"] == \"test\"]])\n",
        "metadata_subset.split.value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "m6On6LGsfV_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell takes a LONG time because it iterates over all data!\n",
        "\n",
        "# save outputs in dictionaries\n",
        "selected_items = {}\n",
        "features_dict = {}\n",
        "errored_ids = []\n",
        "\n",
        "\n",
        "for row in tqdm(metadata_subset.itertuples(), total=len(metadata_subset)):\n",
        "    pass\n",
        "    # check if we've already saved the selected image array\n",
        "    image_array_pth = IMAGE_ARRAY_DIR / f\"{row.uid}.npy\"\n",
        "\n",
        "    if image_array_pth.exists():\n",
        "        with open(image_array_pth, \"rb\") as f:\n",
        "            image_array = np.load(f)\n",
        "\n",
        "        # convert image to 1-dimensional features\n",
        "        image_features = image_to_features(image_array)\n",
        "        features_dict[row.uid] = image_features\n",
        "\n",
        "    # search and load the image array if not\n",
        "    else:\n",
        "        try:\n",
        "            ## QUERY STAC API\n",
        "            # get query ranges for location and date\n",
        "            search_bbox = get_bounding_box(\n",
        "                row.latitude, row.longitude, meter_buffer=50000\n",
        "            )\n",
        "            date_range = get_date_range(row.date, time_buffer_days=15)\n",
        "\n",
        "            # search the planetary computer\n",
        "            search = catalog.search(\n",
        "                collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n",
        "                bbox=search_bbox,\n",
        "                datetime=date_range,\n",
        "            )\n",
        "            items = [item for item in search.get_all_items()]\n",
        "\n",
        "            ## GET BEST IMAGE\n",
        "            if len(items) == 0:\n",
        "                pass\n",
        "            else:\n",
        "                best_item, item_platform, item_date = select_best_item(\n",
        "                    items, row.date, row.latitude, row.longitude\n",
        "                )\n",
        "                # add to dictionary tracking best items\n",
        "                selected_items[row.uid] = {\n",
        "                    \"item_object\": best_item,\n",
        "                    \"item_platform\": item_platform,\n",
        "                    \"item_date\": item_date,\n",
        "                }\n",
        "\n",
        "            ## CONVERT TO FEATURES\n",
        "            # get small bbox just for features\n",
        "            feature_bbox = get_bounding_box(\n",
        "                row.latitude, row.longitude, meter_buffer=100\n",
        "            )\n",
        "\n",
        "            # crop the image\n",
        "            if \"sentinel\" in item_platform.lower():\n",
        "                image_array = crop_sentinel_image(best_item, feature_bbox)\n",
        "            else:\n",
        "                image_array = crop_landsat_image(best_item, feature_bbox)\n",
        "\n",
        "            # save image array so we don't have to rerun\n",
        "            with open(image_array_pth, \"wb\") as f:\n",
        "                np.save(f, image_array)\n",
        "\n",
        "            # convert image to 1-dimensional features\n",
        "            image_features = image_to_features(image_array)\n",
        "            features_dict[row.uid] = image_features\n",
        "\n",
        "        # keep track of any that ran into errors without interrupting the process\n",
        "        except:\n",
        "            errored_ids.append(row.uid)"
      ],
      "metadata": {
        "id": "kxuLtkiJfYnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how many ran into errors\n",
        "print(f\"Could not pull satellite imagery for {len(errored_ids)} samples\")"
      ],
      "metadata": {
        "id": "VbTrnm80fmlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did have a small handful of samples that ran into errors when we tried to pull satellite imagery. For now, we'll omit these from training.\n",
        "\n",
        "Some of these errored samples are also in the test set, and we'll eventually need to generate predictions for them. In the benchmark, we'll do this by taking the average and median of other samples. These are most likely going to be poor predictions, and you'll want to find a way to pull features for every test sample or do more sophisticated imputation for missing values."
      ],
      "metadata": {
        "id": "pvdwg1Ymfnmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bring features into a dataframe\n",
        "image_features = pd.DataFrame(features_dict).T\n",
        "image_features.columns = [\n",
        "    \"red_average\",\n",
        "    \"green_average\",\n",
        "    \"blue_average\",\n",
        "    \"red_median\",\n",
        "    \"green_median\",\n",
        "    \"blue_median\",\n",
        "]\n",
        "image_features.head()"
      ],
      "metadata": {
        "id": "pxCrPpXEfrPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll save out all of our features to a csv! Remember that this includes both the train and test set, just like metadata.csv"
      ],
      "metadata": {
        "id": "DHnuSNacfxvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save out our features!\n",
        "image_features.to_csv(BENCHMARK_DATA_DIR / \"image_features.csv\", index=True)"
      ],
      "metadata": {
        "id": "Iilc8-aefuhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build the model**"
      ],
      "metadata": {
        "id": "W9ah4QEqf4rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build the model**\n",
        "In this benchmark, we'll fit a tree-based model using the LightGBM package. Trees are useful for bringing together image features as well as other types of features, such as something like surface temperature from our ancillary data.\n",
        "\n",
        "# **Split the data**\n",
        "To train our model, we want to separate the data into a \"training\" set and a \"validation\" set. That way we'll have a portion of labelled data that was not used in model training, which can give us a more accurate sense of how our model will perform on the competition test data. Remember that our metadata has both train and test data, so we first have to select just the training data.\n",
        "\n",
        "The competition data train and test sets are split geographically. The US is split into geographic areas, and each geographic area is either entirely in the train set or entirely in the test set. This means that none of the test set water bodies are in the training data, so your model's performance will be measured on unseen locations.\n",
        "\n",
        "In the benchmark we'll use the simplest option of splitting each sample randomly, 1/3 into validation and 2/3 into training. You may want to think about grouping geographically before splitting, to better check how your model will do in new settings."
      ],
      "metadata": {
        "id": "trEufRN5f_kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bring together train labels and features into one dataframe\n",
        "# this ensures the features array and labels array will be in same order\n",
        "train_data = train_labels.merge(\n",
        "    image_features, how=\"inner\", left_on=\"uid\", right_index=True, validate=\"1:1\"\n",
        ")\n",
        "\n",
        "# split into train and validation\n",
        "rng = np.random.RandomState(30)\n",
        "train_data[\"split\"] = rng.choice(\n",
        "    [\"train\", \"validation\"], size=len(train_data), replace=True, p=[0.67, 0.33]\n",
        ")\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "eQqyLUG0f2Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features and labels, and train and validation\n",
        "feature_cols = [\n",
        "    \"red_average\",\n",
        "    \"green_average\",\n",
        "    \"blue_average\",\n",
        "    \"red_median\",\n",
        "    \"green_median\",\n",
        "    \"blue_median\",\n",
        "]\n",
        "target_col = \"severity\"\n",
        "\n",
        "val_set_mask = train_data.split == \"validation\"\n",
        "X_train = train_data.loc[~val_set_mask, feature_cols].values\n",
        "y_train = train_data.loc[~val_set_mask, target_col]\n",
        "X_val = train_data.loc[val_set_mask, feature_cols].values\n",
        "y_val = train_data.loc[val_set_mask, target_col]\n",
        "\n",
        "# flatten label data into 1-d arrays\n",
        "y_train = y_train.values.flatten()\n",
        "y_val = y_val.values.flatten()\n",
        "\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "iEeNem2_g8yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see an example of what the data looks like\n",
        "print(\"X_train[0]:\", X_train[0])\n",
        "print(\"y_train[:10]:\", y_train[:10])"
      ],
      "metadata": {
        "id": "40s-RBu8g93j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build LightGBM Model**\n",
        "Unfortunately, there is a bug in LightGBM that affects MacOS operating systems, and causes a segmentation fault when certain other packages are imported (like the planery_computer). To get around this bug, we'll run the actual model training and prediction in a separate script outside of this notebook."
      ],
      "metadata": {
        "id": "pXez-kIKhHIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save out features\n",
        "x_train_pth = BENCHMARK_DATA_DIR / \"x_train.npy\"\n",
        "x_train_pth.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with open(x_train_pth, \"wb\") as f:\n",
        "    np.save(f, X_train)\n",
        "\n",
        "# save out labels\n",
        "y_train_pth = BENCHMARK_DATA_DIR / \"y_train.npy\"\n",
        "\n",
        "with open(y_train_pth, \"wb\") as f:\n",
        "    np.save(f, y_train)"
      ],
      "metadata": {
        "id": "miFkWrsOhAnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fit model**\n",
        "\n",
        "Our model uses almost all of the default hyperparameters for LightGBM's LGBMClassifier. You may want to experiment with different hyperparameters to find the optimal setup.\n",
        "\n",
        "Below, we use the %%writefile line magic to write out a training script. Then we use ! to run a shell command from our jupyter notebook, and kick off the script.\n",
        "\n",
        "We'll use the typer package to make our script easy to run in the command line."
      ],
      "metadata": {
        "id": "N7zT7BLLhTLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_gbm_model.py\n",
        "import lightgbm as lgb\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "import typer\n",
        "\n",
        "DATA_DIR = Path.cwd().parent / \"data/benchmark\"\n",
        "\n",
        "\n",
        "def main(\n",
        "    features_path=DATA_DIR / \"x_train.npy\",\n",
        "    labels_path=DATA_DIR / \"y_train.npy\",\n",
        "    model_save_path=DATA_DIR / \"lgb_classifier.txt\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a LightGBM model based on training features in features_path and\n",
        "    training labels in labels_path. Save our the trained model to model_save_path\n",
        "    \"\"\"\n",
        "\n",
        "    # load saved features and labels\n",
        "    with open(features_path, \"rb\") as f:\n",
        "        X_train = np.load(f)\n",
        "    with open(labels_path, \"rb\") as f:\n",
        "        y_train = np.load(f)\n",
        "\n",
        "    logger.info(f\"Loaded training features of shape {X_train.shape} from {features_path}\")\n",
        "    logger.info(f\"Loading training labels of shape {y_train.shape} from {labels_path}\")\n",
        "\n",
        "    # instantiate tree model\n",
        "    model = lgb.LGBMClassifier(random_state=10)\n",
        "\n",
        "    # fit model\n",
        "    logger.info(\"Fitting LGBM model\")\n",
        "    model.fit(X_train, y_train)\n",
        "    print(model)\n",
        "\n",
        "    # save out model weights\n",
        "    joblib.dump(model, str(model_save_path))\n",
        "    logger.success(f\"Model weights saved to {model_save_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    typer.run(main)"
      ],
      "metadata": {
        "id": "By9KhKyIhnPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gbm_model.py"
      ],
      "metadata": {
        "id": "LTz2DB-Pht3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate validation set predictions**\n",
        "Let's see how we did on the validation set! First we'll use our new, trained model to generate predictions."
      ],
      "metadata": {
        "id": "ISUbCz4OhztB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save out validation features\n",
        "x_val_pth = BENCHMARK_DATA_DIR / \"x_val.npy\"\n",
        "x_val_pth.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with open(x_val_pth, \"wb\") as f:\n",
        "    np.save(f, X_val)\n",
        "\n",
        "# save out validation labels\n",
        "y_val_pth = BENCHMARK_DATA_DIR / \"y_val.npy\"\n",
        "\n",
        "with open(y_val_pth, \"wb\") as f:\n",
        "    np.save(f, y_val)"
      ],
      "metadata": {
        "id": "QPifIyYthwvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to our model training section, we'll write out a script to generate predictions with our model."
      ],
      "metadata": {
        "id": "wrA0wdFMh-5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict_gbm_model.py\n",
        "import lightgbm as lgb\n",
        "\n",
        "import joblib\n",
        "from loguru import logger\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import typer\n",
        "\n",
        "DATA_DIR = Path.cwd().parent / \"data/benchmark\"\n",
        "\n",
        "\n",
        "def main(\n",
        "    model_weights_path=DATA_DIR / \"lgb_classifier.txt\",\n",
        "    features_path=DATA_DIR / \"x_val.npy\",\n",
        "    preds_save_path=DATA_DIR / \"val_preds.npy\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate predictions with a LightGBM model using weights saved at model_weights_path\n",
        "    and features saved at features_path. Save out predictions to preds_save_path.\n",
        "    \"\"\"\n",
        "    # load model weights\n",
        "    lgb_model = joblib.load(model_weights_path)\n",
        "    logger.info(f\"Loaded model {lgb_model} from {model_weights_path}\")\n",
        "\n",
        "    # load the features\n",
        "    with open(features_path, \"rb\") as f:\n",
        "        X_val = np.load(f)\n",
        "    logger.info(f\"Loaded features of shape {X_val.shape} from {features_path}\")\n",
        "\n",
        "    # generate predictions\n",
        "    preds = lgb_model.predict(X_val)\n",
        "\n",
        "    # save out predictions\n",
        "    with open(preds_save_path, \"wb\") as f:\n",
        "        np.save(f, preds)\n",
        "    logger.success(f\"Predictions saved to {preds_save_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    typer.run(main)"
      ],
      "metadata": {
        "id": "pF_hQ9sgh8k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict_gbm_model.py"
      ],
      "metadata": {
        "id": "-Y-yMYowiRBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate performance**\n",
        "\n",
        "The metric for this competition is Root Mean Squared Error (RMSE). RMSE is the square root of the mean of squared differences between estimated and observed values. This is an error metric, so a lower value is better. RMSE is implemented in scikit-learn, with the squared parameter set to False.\n",
        "\n",
        "RMSE=1N∑i=0N(yi−yi^)2−−−−−−−−−−−−−⎷\n",
        "where:\n",
        "\n",
        "yi^\n",
        " is the predicted severity level for the |i\n",
        "|th sample\n",
        "yi\n",
        " is the actual severity level for the |i\n",
        "|th sample\n",
        "N\n",
        " is the number of samples\n",
        "RMSE will be calculated separately for each of the four regions of the U.S. shown below: West, Midwest, South, and Northeast. Your final score will be the average of all region-specific RMSEs. Region is included in train_labels.csv and submission_format.csv.\n",
        "\n",
        "Final score=West RMSE+Midwest RMSE+South RMSE+Northeast RMSE4\n",
        "See the Problem description page for details."
      ],
      "metadata": {
        "id": "0m8kUP6IiUyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_pth = BENCHMARK_DATA_DIR / \"val_preds.npy\"\n",
        "with open(preds_pth, \"rb\") as f:\n",
        "    val_preds = np.load(f)"
      ],
      "metadata": {
        "id": "wZJk_ew3idMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds[:10]"
      ],
      "metadata": {
        "id": "LAw-asvkij1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(val_preds).value_counts().sort_index()"
      ],
      "metadata": {
        "id": "-rbDGwH4imZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add these back to our train_data dataframe to see which region each prediction is in, and get RMSE by region for the validation set."
      ],
      "metadata": {
        "id": "MYDl-1IPis7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the validation part of the training data\n",
        "val_set = train_data[train_data.split == \"validation\"][\n",
        "    [\"uid\", \"region\", \"severity\"]\n",
        "].copy()\n",
        "val_set[\"pred\"] = val_preds\n",
        "\n",
        "val_set.head()"
      ],
      "metadata": {
        "id": "O1OYFclYipUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_scores = []\n",
        "for region in val_set.region.unique():\n",
        "    sub = val_set[val_set.region == region]\n",
        "    region_rmse = mean_squared_error(sub.severity, sub.pred, squared=False)\n",
        "    print(f\"RMSE for {region} (n={len(sub)}): {round(region_rmse, 4)}\")\n",
        "    region_scores.append(region_rmse)\n",
        "\n",
        "overall_rmse = np.mean(region_scores)\n",
        "print(f\"Final score: {overall_rmse}\")"
      ],
      "metadata": {
        "id": "tW7-WF2viwiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a good start, but there's plenty of room for improvement in your solution!\n",
        "\n",
        "Out of curiosity, let's see how different our score would be if we calculated RMSE across all of the validation data, without calculating for each region independently. The overall RMSE is slightly lower without first aggregating by region."
      ],
      "metadata": {
        "id": "vdHbN1LQi_qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what's our RMSE across all validation data points?\n",
        "mean_squared_error(y_val, val_preds, squared=False)"
      ],
      "metadata": {
        "id": "8QO63o8wjBRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we'll do a small amount of comparison between our predicted values and the actual values. One good strategy for model improvement is error analysis, which means looking into scenarios that our model got wrong and identifying any patterns."
      ],
      "metadata": {
        "id": "8YkKYtdwjH3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many times did each severity level show up in our predictions vs. the actual values?\n",
        "val_results = pd.DataFrame({\"pred\": val_preds, \"actual\": y_val})\n",
        "\n",
        "pd.concat(\n",
        "    [\n",
        "        val_results.pred.value_counts().sort_index().rename(\"predicted\"),\n",
        "        val_results.actual.value_counts().sort_index().rename(\"actual\"),\n",
        "    ],\n",
        "    axis=1,\n",
        ").rename_axis(\"severity_level_count\")"
      ],
      "metadata": {
        "id": "gEyowaVHjEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our predictions miss a lot of the higher-severity points, and we predicted a severity of 1 too often."
      ],
      "metadata": {
        "id": "56vZB1_ujOzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate a submission**\n",
        "\n",
        "To generate a submission, we'll download the submission format and replace the severity values with our own predictions for severity.\n",
        "\n",
        "Process test feature data\n",
        "Like in the model training section, we'll save out our test set features and run prediction in a separate script, to get around a bug in LightGBM."
      ],
      "metadata": {
        "id": "NhBSdiNtjSRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the image features for the test set\n",
        "test_features = submission_format.join(image_features, how=\"left\", validate=\"1:1\")\n",
        "\n",
        "# make sure our features are in the same order as the submission format\n",
        "assert (test_features.index == submission_format.index).all()\n",
        "\n",
        "test_features.head()"
      ],
      "metadata": {
        "id": "ftQ_t4u9jMCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.isna().sum()"
      ],
      "metadata": {
        "id": "VNkzfLe5jY2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we noted when we pulled satellite imagery for the full training set, some of our samples ran into errors when we queried the Planetary Computer.\n",
        "\n",
        "For now, we'll fill in these missing values with the average or median of all of our other test samples. These are most likely not very good features for these samples, and leave lots of room for improvement!"
      ],
      "metadata": {
        "id": "LWnnhTosjeix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in missing values\n",
        "for avg_col in [\"red_average\", \"green_average\", \"blue_average\"]:\n",
        "    test_features[avg_col] = test_features[avg_col].fillna(\n",
        "        test_features[avg_col].mean()\n",
        "    )\n",
        "for median_col in [\"red_median\", \"green_median\", \"blue_median\"]:\n",
        "    test_features[median_col] = test_features[median_col].fillna(\n",
        "        test_features[median_col].median()\n",
        "    )"
      ],
      "metadata": {
        "id": "sqK2Pm_sjbzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select feature columns\n",
        "feature_cols = [\n",
        "    \"red_average\",\n",
        "    \"green_average\",\n",
        "    \"blue_average\",\n",
        "    \"red_median\",\n",
        "    \"green_median\",\n",
        "    \"blue_median\",\n",
        "]\n",
        "\n",
        "X_test = test_features[feature_cols].values\n",
        "\n",
        "print(X_test.shape)\n",
        "\n",
        "X_test[1]"
      ],
      "metadata": {
        "id": "Dnk7hyfcjiNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save out test features\n",
        "x_test_pth = BENCHMARK_DATA_DIR / \"x_test.npy\"\n",
        "with open(x_test_pth, \"wb\") as f:\n",
        "    np.save(f, X_test)"
      ],
      "metadata": {
        "id": "PliOtMYGjlNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict**\n",
        "We'll use our same predict_gbm_model.py script from above, but we'll specify different paths for features_path and preds_save_path"
      ],
      "metadata": {
        "id": "1X8XsCksjrkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_pth = BENCHMARK_DATA_DIR / \"test_preds.npy\""
      ],
      "metadata": {
        "id": "a8nEBJaIjoPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict_gbm_model.py --features-path {x_test_pth} --preds-save-path {test_preds_pth}"
      ],
      "metadata": {
        "id": "3jv35mXRjwrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finalize format**"
      ],
      "metadata": {
        "id": "Kbw0S9epj2pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load our predictions\n",
        "with open(test_preds_pth, \"rb\") as f:\n",
        "    test_preds = np.load(f)"
      ],
      "metadata": {
        "id": "8uxhneVJjz7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our predictions are already in the same order as the submission format, since we used the submission format indices to sort our test features."
      ],
      "metadata": {
        "id": "_OsTDZEqj8eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = submission_format.copy()\n",
        "submission[\"severity\"] = test_preds\n",
        "\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "iJT-zKsfj6f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that for our submission to be scored correctly, our severity column needs to be integers."
      ],
      "metadata": {
        "id": "42WEdazGkC8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save out our formatted submission\n",
        "submission_save_path = BENCHMARK_DATA_DIR / \"submission.csv\"\n",
        "submission.to_csv(submission_save_path, index=True)"
      ],
      "metadata": {
        "id": "jVPEPnxPkAdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure our saved csv looks correct\n",
        "!cat {submission_save_path} | head -5"
      ],
      "metadata": {
        "id": "MHXcyuOkkHBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload submission**\n",
        "\n",
        "We can now head over to the competition submissions page to upload our code and get our model's RMSE!"
      ],
      "metadata": {
        "id": "LqJAhqoFkOl7"
      }
    }
  ]
}